
<html lang="en-US">
<!--<![endif]-->
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />
<title>Thomas Moreau</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<!--[if lt IE 9]>
<script src="http://dpkingma.com/wordpress/wp-content/themes/twentytwelve/js/html5.js" type="text/javascript"></script>
<![endif]-->
<script src="https://use.fontawesome.com/ae904b9937.js"></script>



<link rel="stylesheet" type="text/css" href="/css/style.css" />
<link rel="stylesheet" href="/css/academicons.css"/>
<script type="text/javascript" src="/javascript/lib.js"></script>
<!-- highlight current page in navigation -->
<script>
$(function(){
  $('a').each(function() {
    if ($(this).prop('href') == window.location.href) {
      $(this).parent().addClass('current-menu-item');
    }
  });
});
</script>
    
</head>

<body class="home">
<div id="sidebar">
    <header id="masthead" class="site-header" role="banner">
        <hgroup>
            <img class="profile_img" src="https://s.gravatar.com/avatar/1c7c7939d1173cd6f455ce4313e831c5?s=150&r=x" />
            <div class="info">
                <h2 class="site-title"><a href="about">Thomas Moreau</a></h2>
                <h4 class="site-description">CMLA - ENS Paris Saclay </h4>
                <h5 class="email"> thomas.moreau [AT] cmla.ens-cachan.fr</h5>
            </div>
        </hgroup>

        <nav id="site-navigation" class="main-navigation" role="navigation">
            <div class="menu-menu-1-container"><ul id="menu-nav" class="menu-nav">
                <li class="nav-menu-item"><a href="about">About</a></li>
                <li class="nav-menu-item"><a href="publications">Publications</a></li>
                <li class="nav-menu-item"><a href="talks">Talks and Videos</a></li>
                <li class="nav-menu-item"><a href="oss">Open Source Software</a></li>
            </ul></div>
        </nav><!-- #site-navigation -->
        <div id="social-media">
            <a href="https://github.com/tomMoral">
                <i class="fa fa-github fa-3x"></i></a>
            <a href="https://scholar.google.fr/citations?user=HEO_PsAAAAAJ">
                <i class="ai ai-google-scholar ai-3x"></i></a>
            <a href="https://stackoverflow.com/story/tomMoral">
                <i class="fa fa-stack-overflow fa-3x"></i></a>
            <a href="https://linkedin.com/in/tomMoral">
                <i class="fa fa-linkedin fa-3x"></i></a>
        </div>
    </header><!-- #masthead -->
</div>
</div>

<div id="page">
    <div class="page-content">
    <article class="inner-text">
    <header class="entry-header">
        <h1 class="entry-title">About me</h1>
    </header>

    <div class="entry-content">
	<p>
	    I am a PhD student at Centre de Mathématiques et de leurs applications (CMLA), ENS Paris-Saclay, since Fall 2014.
	    My PhD subject is <i>Convolutional Sparse Representations -- application to physiological signals and interpretability
            for Deep Learning</i>.
	</p>

	<p>
            My research interests touch several areas of Machine Learning, Signal Processing and High-Dimensional Statistics.
	    In particular, I have been working on Convolutional Dictionary Learning, studying both their computational aspects
            and their possible application to pattern analysis. I am also studying on theoretical properties of learned
            optimization algorithms.
	</p>
	<p>
	    Before enrolling in my PhD program, I did a MsC at Ecole Normale Superieure de Cachan in Applied Mathematics (MVA)
            and a MsC at Ecole Polytechnique in both Applied Mathematics and Computer Science.
	</p>

	<h3>Latest publication and projects</h3>
        
    <div class="publication">
        <div class="project_item">
            <span class="title">Understanding the Learned Iterative Soft Thresholding Algorithm with matrix factorization </span>
            <span style="width:2em;"></span>
            
            <a href="https://arxiv.org/pdf/1706.01338.pdf">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>
            
            
            <br>
            <span class="authors">Thomas Moreau, Joan Bruna,</span>
            <span class="date">Jun 2017,</span>
            
                <span><i>preprint Arxiv</i></span>
            
 
        </div>

        <input type="checkbox" class="btnCtrl" id="pub7"/>
        <label class="btn display-status" for="pub7">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            
            <div class="summary">
                This paper aims to extend the results from our previous paper studying the mechanisms of LISTA and gives an acceleration certificate for generic dictionaries.
            </div>
            <div class="details">
                Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in Gregor and Le Cun (2010), which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.</br>

In this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the ℓ1 ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.
            </div>
            
        </div>
    </div>
            <div class="talk">
        
    	<div class="talk_item">
            <span class="title">Understanding Trainable Sparse Coding with Matrix Factorization </span>
            <span style="width:2em;"></span>
            
            <a href="/talks/17_11_Google_ZRH.pdf">
                
                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>
            
            
            <br>
            <span class="date">Mon Nov 2017,</span>
            <span class="conference">At <i>Tech talk - Google Zurich</i></span>
 
        </div>

        <input type="checkbox" class="btnCtrl" id="talk3"/>
        <label class="btn display-status" for="talk3">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                In this talk, we provide elements explaining why Learned ISTA is able to accelerate the LASSO resolution. This vision of optimization algorithms in the neural network framework could be used to link sparse representations and neural networks.
            </div>
            <div class="details">
                Optimization algorithms for sparse coding can be viewed in the light of the neural network framework. Using this framework, it is possible to design trainable networks which accelerate the resolution of an optimization problem on a given distribution, as it has been shown with the Learned ISTA network, proposed by Gregor & Le Cun (2010).</br></br>
In this talk, we provide elements explaining why the acceleration is possible in the case of ISTA. We show that the resolution of sparse coding can be accelerated compared to ISTA when the design matrix admits a quasi-diagonal factorization with sparse eigenspaces. The resulting algorithm has the same convergence rate but an improved constant factor. Then we show under which conditions such factorization is possible with high probability for generic Gaussian dictionaries. Finally, we design neural networks which compute this algorithm and show that they are a re-parametrization of LISTA. Thus, the performance of LISTA are at least as good as this algorithm. We conclude by designing adverse examples for our factorization based algorithm and show that LISTA also fails to accelerate on these cases, proving that this mechanism plays a role in LISTA acceleration.
            </div>
        </div>
        
    </div>

        <div class="project">
    <div class="project_item">
        <span class="title">Loky </span>
        <a href="https://github.com/tomMoral/loky"><i class="fa fa-github-alt" aria-hidden="true"></i>
</a>
        <span class="date">Nov 2016</span>

    </div>

    <input type="checkbox" class="btnCtrl" id="proj1"/>
    <label class="btn display-status" for="proj1">
        <i class="fa fa-plus-circle plus"></i>
        <i class="fa fa-minus-circle minus"></i>
    </label>
    <div class="description">
        
        <span class="summary">
            The aim of this project is to provide a robust, cross-platform and cross-version implementation of the <span class="highlightcode">ProcessPoolExecutor</span> class of <span class="highlightcode">concurrent.futures</span>.
        </span>
        <div class="details">
            The aim of this project is to provide a robust, cross-platform and cross-version implementation of the <span class="highlightcode">ProcessPoolExecutor</span> class of <span class="highlightcode">concurrent.futures</span>. It features:</br>
<ul> <li> <b>Deadlock free implementation</b>: one of the major concern in standard multiprocessing and concurrent.futures libraries is the ability of the Pool/Executor to handle crashes of worker processes. This library intends to fix those possible deadlocks and send back meaningful errors.</li></br>
<li><b>Consistent spawn behavior</b>: All processes are started using fork/exec on POSIX systems. This ensures safer interactions with third party libraries.</li></br>
<li><b>Reusable executor</b>: strategy to avoid respawning a complete executor every time. A singleton pool can be reused (and dynamically resized if necessary) across consecutive calls to limit spawning and shutdown overhead. The worker processes can be shutdown automatically after a configurable idling timeout to free system resources.</li></ul></br>
</br>
<i>python, multiprocessing, parallel computing</i>
        </div>
        
    </div>
</div>
    </div><!-- .entry-content -->
</article><!-- #post -->

    </div>
</div><!-- #page -->

<div style="display:none">
</div>

</body>

<script type="text/javascript">
    window.onresize = function(){
        var sidebar = document.getElementById("sidebar");
        var page = document.getElementById("page");
        page.style.setProperty("min-height", sidebar.clientHeight);
    }
    window.onresize()
</script>
</html>
